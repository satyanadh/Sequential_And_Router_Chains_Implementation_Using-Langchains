{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**TASK - 1**"
      ],
      "metadata": {
        "id": "HDx55kAJNFG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXXxZjPtNC7x"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# account for deprecation of LLM model\n",
        "import datetime\n",
        "# Get the current date\n",
        "current_date = datetime.datetime.now().date()\n",
        "\n",
        "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
        "target_date = datetime.date(2024, 6, 12)\n",
        "\n",
        "# Set the model variable based on the current date\n",
        "if current_date > target_date:\n",
        "    llm_model = \"gpt-3.5-turbo\"\n",
        "else:\n",
        "    llm_model = \"gpt-3.5-turbo-0301\""
      ],
      "metadata": {
        "id": "QztbMTA3NH3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai==0.28.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y19IlismNLjX",
        "outputId": "2090646d-2b06-40ee-dae3-0aa3fd7773ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.331)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.60)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "cXvGZUR0NPhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Assuming this is the correct import based on your code\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "Fdvovz4sNSph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
      ],
      "metadata": {
        "id": "jsE1c1WcNVu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Story Title Generation\n",
        "story_title_template = ChatPromptTemplate.from_template(\n",
        "    \"Create a captivating story title about an {story}\"\n",
        ")\n",
        "story_title_chain = LLMChain(llm=llm, prompt=story_title_template)\n",
        "\n",
        "story = \"ancient hidden treasure in Egypt\"\n",
        "story_title_result = story_title_chain.run(story)\n",
        "\n",
        "print(f\"Generated Story Title: {story_title_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT_qkVvkNitu",
        "outputId": "5ff5697f-bffc-4b07-f86e-a3ed7ee4966d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story Title: \"Tomb of the Pharaohs: The Quest for Egypt's Hidden Fortune\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt for a poem\n",
        "poem_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Compose a poem about the {poem}\"\n",
        ")\n",
        "poem_chain = LLMChain(llm=llm, prompt=poem_prompt)\n",
        "\n",
        "poem = \"beauty and serenity of a mountain lake at dawn\"\n",
        "poem_result = poem_chain.run(poem)\n",
        "\n",
        "print(f\"Generated Poem:\\n{poem_result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBzGgbLcQ7cO",
        "outputId": "fd23c070-82c7-4254-aae7-63d27c052b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Poem:\n",
            "The sky awakens with a blush of rose,\n",
            "the morning dew shimmers on the leaves,\n",
            "the quiet breeze whispers secrets untold,\n",
            "and the mountain lake lies still like a dream.\n",
            "\n",
            "The misty veil slowly lifts its veil,\n",
            "revealing a glistening, glass-like sheen,\n",
            "a tranquil mirror reflecting the skies,\n",
            "and the mountains that rise in between.\n",
            "\n",
            "The water's edge, touched by golden hues,\n",
            "the birds chirping softly, a symphony in tune,\n",
            "the gentle ripples, a melody so sweet,\n",
            "and the calmness of nature in its retreat.\n",
            "\n",
            "The tranquility of the mountain lake at dawn,\n",
            "an ode to beauty, a song to the soul,\n",
            "a serene canvas of quietude and calm,\n",
            "a moment of stillness that makes us whole.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt for a joke\n",
        "joke_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Create a joke about a {joke}\"\n",
        ")\n",
        "joke_chain = LLMChain(llm=llm, prompt=joke_prompt)\n",
        "\n",
        "joke = \"photon who didn't need to check her luggage\"\n",
        "joke_result = joke_chain.run(joke)\n",
        "\n",
        "print(f\"Generated Joke: {joke_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLnp46LHQjqE",
        "outputId": "353562e0-755a-419c-864a-05a47527a963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Joke: Why did the photon skip checking her luggage? Because she was traveling light!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK - 2**"
      ],
      "metadata": {
        "id": "40ggqrW8TzRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "zcl077rxWfzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Create a product description for the following item:\"\n",
        "    \"\\n\\n{product_details}\"\n",
        ")\n",
        "# chain 1: input= product_details and output= product_description\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key=\"product_description\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "6OdEyWU5bu5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Craft a social media post to promote the following product:\"\n",
        "    \"\\n\\n{product_description}\"\n",
        ")\n",
        "# chain 2: input= product_description and output= social_media_post\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"social_media_post\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "wXYI3ytdbw-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_chain: input= product_details\n",
        "# and output= product_description, social_media_post\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two],\n",
        "    input_variables=[\"product_details\"],\n",
        "    output_variables=[\"product_description\", \"social_media_post\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "OO19mw_PcPcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the product details\n",
        "product_details = \"Eco-friendly yoga mat, made from natural rubber, 5mm thickness, non-slip surface.\"\n",
        "\n",
        "# Run the SequentialChain with the prompts\n",
        "results = overall_chain(product_details)\n",
        "\n",
        "# Display the generated product description\n",
        "print(f\"Product Description:\\n{results['product_description']}\")\n",
        "# Display the generated marketing content\n",
        "print(f\"Social Media Post:\\n{results['social_media_post']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbxPswS6W2rC",
        "outputId": "e06fd5a5-241f-40a7-f8b4-c6a3f96ebcd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Product Description:\n",
            "Introducing our eco-friendly yoga mat, crafted with the environment in mind. Made with natural rubber, this mat provides a comfortable, non-slip surface for your yoga practice. At 5mm thickness and with a durable construction, it provides the perfect level of cushioning for your joints, while still being lightweight and easy to transport. Not only will you be practicing yoga with ease, but you'll also be doing your part for the planet. Choose our eco-friendly yoga mat and feel good about supporting a sustainable future.\n",
            "Social Media Post:\n",
            "ðŸŒ¿ Introducing our eco-friendly yoga mat â€“ the perfect choice for yogis who care about the environment! ðŸ™ðŸŒ Made with natural rubber, this mat delivers the perfect combination of comfort and sustainability. ðŸ’ª With a non-slip surface, 5mm thickness, and lightweight construction, you'll be able to practice yoga with ease. ðŸ§˜â€â™€ï¸ Plus, you'll feel great knowing you're doing your part for the planet! ðŸŒ± Choose our eco-friendly yoga mat and support a sustainable future. #ecofriendlyyoga #sustainability #yogamat #naturalrubber #non-slip #comfortable #lightweight #cushioning #joints #yoga #planetfriendly #supportthecause #environmentallyfriendly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK - 3**"
      ],
      "metadata": {
        "id": "nyxxK8ayezdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "translation_prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following text to English:\"\n",
        "    \"\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "# chain 1: input= text and output= Translated_Text\n",
        "chain_one = LLMChain(llm=llm, prompt=translation_prompt_template,\n",
        "                     output_key=\"Translated_Text\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "ehAkcMoUe2jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Summarize the following topic in English with up to 100 words:\"\n",
        "    \"\\n\\n{Translated_Text}\"\n",
        ")\n",
        "\n",
        "# chain 2: input= Translated_Text and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=summarization_prompt_template,\n",
        "                     output_key=\"Summary\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "yp7V_8QWe6vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analysis_prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Analyze the sentiment of the following text:\"\n",
        "    \"\\n\\n{Summary}\"\n",
        ")\n",
        "\n",
        "# chain 3: input= Translated_Text and output= summary\n",
        "chain_three = LLMChain(llm=llm, prompt=sentiment_analysis_prompt_template,\n",
        "                     output_key=\"Sentiment_Summary\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "g7BaawAue88R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_chain: input= product_details\n",
        "# and output= product_description, social_media_post\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three],\n",
        "    input_variables=[\"text\"],\n",
        "    output_variables=[\"Translated_Text\",\"Summary\",\"Sentiment_Summary\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "It1EgVIQg93E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The non-English text we want to process\n",
        "non_english_text = \"Il est crucial pour l'avenir de notre planÃ¨te de prendre des mesures immÃ©diates contre le changement climatique.\"\n",
        "\n",
        "# Run the Regular Sequential Chain with the provided text\n",
        "results = overall_chain(non_english_text)\n",
        "\n",
        "# Display the outputs\n",
        "print(f\"Translated Text:\\n{results['Translated_Text']}\\n\")\n",
        "print(f\"Text Summary:\\n{results['Summary']}\\n\")\n",
        "print(f\"Sentiment Analysis Results:\\n{results['Sentiment_Summary']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFHHYvy5hYXz",
        "outputId": "87bd0228-a2d5-4088-f6a1-ef21c3f1d74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Translated Text:\n",
            "It is crucial for the future of our planet to take immediate action against climate change.\n",
            "\n",
            "Text Summary:\n",
            "Taking immediate action against climate change is essential for the survival of our planet.\n",
            "\n",
            "Sentiment Analysis Results:\n",
            "Sentiment: Positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK - 4**"
      ],
      "metadata": {
        "id": "VBqeG8MRjbAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "bxMVLKmgoW3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, model=llm_model)"
      ],
      "metadata": {
        "id": "ZT3jy-opoYqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flight_booking_template = \"\"\"You are an expert in flight booking services. Here is a customer inquiry:\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "hotel_reservation_template = \"\"\"You are an expert in hotel reservation services. Here is a customer inquiry:\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "car_rental_template = \"\"\"You are an expert in car rental services. Here is a customer inquiry:\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "gEU3b29Z27oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"flight booking\",\n",
        "        \"description\": \"Good for answering questions about flight booking services\",\n",
        "        \"prompt_template\": flight_booking_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"hotel reservation\",\n",
        "        \"description\": \"Good for answering questions about hotel reservation\",\n",
        "        \"prompt_template\": hotel_reservation_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"car rentals\",\n",
        "        \"description\": \"Good for answering questions about car rentals\",\n",
        "        \"prompt_template\": car_rental_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "H3TFLirc3p4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "I0pxmBGW4Sb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "6YYhxQAt4iNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "xDOcw7Jy4z6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "RvPohBzR40_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "O6Hd1pXa45B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"I need to change the date of my flight booking.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "dhUIQE1z5I6G",
        "outputId": "164f4503-918d-410c-b409-e044f5697dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "flight booking: {'input': 'I need to change the date of my flight booking.'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Thank you for reaching out to us. We would be happy to assist you with changing the date of your flight booking. Please provide us with your booking reference number and the new date you would like to travel. We will check the availability and provide you with the options available. Please note that there may be a change fee and fare difference depending on the airline's policy and the fare type you have purchased. We look forward to hearing back from you soon.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"I would like to make a reservation at a beachfront hotel.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "32sFJxEP5etc",
        "outputId": "ccc8e613-4da0-4f0e-9a7b-1e62d648a53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "hotel reservation: {'input': 'I would like to make a reservation at a beachfront hotel.'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Great! We would be happy to assist you with your reservation. Can you please provide us with some additional information such as the dates of your stay, the number of guests, and any specific amenities or room preferences you may have? This will help us to find the perfect beachfront hotel that meets your needs and preferences. Thank you!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Can you help me hire a car for next week?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "w0Cbk9_A5e85",
        "outputId": "63c75b87-32f8-417b-a925-ee177bef9058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "car rentals: {'input': 'Can you help me hire a car for next week?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Absolutely! We would be happy to assist you in finding the perfect car rental for your needs. Can you please provide us with some more information such as the location, dates, and type of car you are looking for? This will help us narrow down the options and provide you with the best possible recommendations. Thank you!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK - 5**"
      ],
      "metadata": {
        "id": "MXg2YbuvuJ-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this Task, I worked on making a tool that takes a long customer review and makes it short, then figures out if the review is good, bad, or just okay. I find this is intresting because:\n",
        "\n",
        "1. **Lots to Read**: There are tons and tons of reviews online, and no one has the time to read them all. This tool can help businesses understand what their customers are saying quickly.\n",
        "\n",
        "2. **Useful for Business**: Knowing what customers like or don't like can help a business do better. This tool tells them what's working and what's not without reading every single review.\n",
        "\n",
        "3. **Tricky Language**: Sometimes people write reviews with jokes or hidden meanings, and it's hard to tell if they're happy or not. It's a fun challenge to build something that gets the real meaning.\n",
        "\n",
        "4. **Helps Businesses React Fast**: When businesses know what customers are saying right away, they can fix problems quicker or do more of what people love.\n",
        "\n",
        "5. **Smart AI**: It's exciting to see how smart AI is getting, understanding words like a human would, and that's what this tool tries to do.\n",
        "\n",
        "Putting this tool together shows off how AI can be super helpful in understanding all the complicated stuff people say. It's like having a super-fast reader who also tells you how people feel."
      ],
      "metadata": {
        "id": "iC9fZoc5qjKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "# Define the prompt for summarization\n",
        "summarization_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Summarize the following detailed customer review:\"\n",
        "    \"\\n\\n{customer_review}\"\n",
        ")\n",
        "\n",
        "# chain 1: input= customer_review and output= review_summary\n",
        "chain_one = LLMChain(llm=llm, prompt=summarization_prompt,\n",
        "                     output_key=\"review_summary\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "x_yRdAb9SF1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt for sentiment analysis\n",
        "sentiment_analysis_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Determine the sentiment of this review summary:\"\n",
        "    \"\\n\\n{review_summary}\"\n",
        ")\n",
        "\n",
        "# chain 2: input= review_summary and output= sentiment\n",
        "chain_two = LLMChain(llm=llm, prompt=sentiment_analysis_prompt,\n",
        "                     output_key=\"sentiment\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "fqbCuQ6USXb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# sequential_chain: input= customer_review\n",
        "# and output= review_summary, sentiment\n",
        "\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two],\n",
        "    input_variables=[\"customer_review\"],\n",
        "    output_variables=[\"review_summary\",\"sentiment\"],\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "wIk3my8xuMQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customer review input\n",
        "customer_review = \"I recently purchased the new smartphone brand X1000, and I must say the experience has been disappointing. \\\n",
        "The phone's design is sleek and it operates quite fast, but the battery life is abysmal, and the camera quality is worse than advertised. \\\n",
        "I had high hopes for this phone, but it falls short in key areas that are important to me.\"\n",
        "\n",
        "# Run the custom sequential chain with the sample review\n",
        "results = sequential_chain(customer_review)\n",
        "\n",
        "# Display the output results\n",
        "print(f\"Review Summary:\\n{results['review_summary']}\")\n",
        "print(f\"Sentiment Analysis:\\n{results['sentiment']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lj27hyVWXyk",
        "outputId": "dbb6fc4b-71ce-4bf6-aae4-7c799eea5c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Review Summary:\n",
            "The customer is disappointed with the new smartphone brand X1000, citing poor battery life and camera quality despite a sleek design and fast operation. The phone falls short in important areas for the customer.\n",
            "Sentiment Analysis:\n",
            "The sentiment of the review summary is negative.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review = (\n",
        "    \"I just stayed at the Seaside Hotel and my experience was mixed. The room had a breathtaking view \"\n",
        "    \"of the ocean and the bed was incredibly comfortable. However, the service at the reception was less \"\n",
        "    \"than welcoming, and the food quality at their restaurant left a lot to be desired. For the price I paid, \"\n",
        "    \"I expected top-notch service and dining options. The amenities like the pool and spa made up for some \"\n",
        "    \"of the downsides, but overall, I'm not sure I would return.\"\n",
        ")\n",
        "\n",
        "# Run the custom sequential chain with the sample review\n",
        "results = sequential_chain(customer_review)\n",
        "\n",
        "# Display the output results\n",
        "print(f\"Review Summary:\\n{results['review_summary']}\")\n",
        "print(f\"Sentiment Analysis:\\n{results['sentiment']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EUyBmIOWEFP",
        "outputId": "f9e2c8bb-9e5b-4530-e868-1b87ad87d150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Review Summary:\n",
            "The Seaside Hotel had great views and comfortable beds, but the reception service was unfriendly and the food quality was poor. The price was high and the reviewer expected better service and dining options. The amenities were a nice addition, but the overall experience was mixed and the reviewer may not return.\n",
            "Sentiment Analysis:\n",
            "The sentiment of this review summary is negative.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK - 6**"
      ],
      "metadata": {
        "id": "iHGZOi616_aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scenario:**\n",
        "Develop a system that processes customer feedback for a restaurant to understand their dining experience better. The system should categorize feedback, summarize the sentiment, and identify areas for improvement.\n",
        "\n",
        "**Objectives:**\n",
        "\n",
        "Feedback Categorization Chain: Classify feedback into categories such as service, food quality, ambiance, and price.\n",
        "\n",
        "Sentiment Summarization Chain: Summarize the overall sentiment from the categorized feedback to gauge customer satisfaction.\n",
        "\n",
        "Improvement Identification Chain: Highlight specific suggestions or recurring issues that could inform potential improvements.\n",
        "\n",
        "**Expected Outcomes:**\n",
        "\n",
        "Restaurant managers receive categorized summaries of customer feedback.\n",
        "They gain quick insights into overall sentiment trends.\n",
        "\n",
        "They identify specific actionable items for improving the customer experience."
      ],
      "metadata": {
        "id": "7T-k9IIzd_y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "# Chain 1: Feedback Categorization\n",
        "feedback_categorization_prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Classify and categorize the following customer feedback:\"\n",
        "    \"\\n\\n {feedback}\"\n",
        ")\n",
        "\n",
        "# chain 1: input= feedback and output= categorized_feedback\n",
        "chain_one = LLMChain(llm=llm, prompt=feedback_categorization_prompt_template,\n",
        "                     output_key=\"categorized_feedback\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "wDA36qHeaE2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain 2: Sentiment Summarization\n",
        "sentiment_summarization_prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Summarize the sentiment of the following categorized feedback:\"\n",
        "    \"\\n\\n {categorized_feedback}\"\n",
        ")\n",
        "\n",
        "# chain 2: input= categorized_feedback and output= categorized_feedback\n",
        "chain_two = LLMChain(llm=llm, prompt=sentiment_summarization_prompt_template,\n",
        "                     output_key=\"feedback_summary\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "noK1qdrEal7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain 3: Improvement Identification\n",
        "improvement_identification_prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Identify key areas for improvement based on the following feedback summary:\"\n",
        "    \"\\n\\n {feedback_summary}\"\n",
        ")\n",
        "\n",
        "# chain 3: input= feedback_summary and output= improvement_suggestions\n",
        "chain_three = LLMChain(llm=llm, prompt=improvement_identification_prompt_template,\n",
        "                     output_key=\"improvement_suggestions\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "7By8aKPYbBVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sequential_chain: input= feedback\n",
        "# and output= categorized_feedback, feedback_summary, improvement_suggestions\n",
        "\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three],\n",
        "    input_variables=[\"feedback\"],\n",
        "    output_variables=[\"categorized_feedback\",\"feedback_summary\",\"improvement_suggestions\"],\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "CAFxohjKbphq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customer review input\n",
        "customer_feedback = (\n",
        "    \"The steak was cooked to perfection, and the wine selection impressed me. \"\n",
        "    \"However, the waiter seemed disinterested, and it took a long time to get the bill. \"\n",
        "    \"The atmosphere was cozy, but the prices are a bit too high for what you get.\"\n",
        ")\n",
        "\n",
        "# Run the custom sequential chain with the sample review\n",
        "results = sequential_chain(customer_feedback)\n",
        "\n",
        "# Display the output results\n",
        "print(f\"Categorized Feedback:\\n{results['categorized_feedback']}\")\n",
        "print(f\"Feedback Sentiment Summary:\\n{results['feedback_summary']}\")\n",
        "print(f\"Improvement Suggestions:\\n{results['improvement_suggestions']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrXOK5_HcVMo",
        "outputId": "ffe1cbfb-5818-4239-ce77-74b62c4ca58d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Categorized Feedback:\n",
            "Classification: Customer Feedback\n",
            "Categories:\n",
            "- Food quality: Positive\n",
            "- Drink selection: Positive \n",
            "- Service: Negative \n",
            "- Waiting time: Negative \n",
            "- Atmosphere: Positive \n",
            "- Value for money: Negative\n",
            "Feedback Sentiment Summary:\n",
            "Overall, the customer feedback is mixed. The food quality and drink selection are positive, while the service and waiting time are negative. The atmosphere is positive, but the value for money is negative.\n",
            "Improvement Suggestions:\n",
            "Areas for improvement:\n",
            "1. Service: The negative feedback on service suggests that the restaurant needs to focus on improving its customer service experience. This may include training staff to be more responsive, attentive, and friendly towards customers.\n",
            "\n",
            "2. Waiting time: The negative feedback on waiting time suggests that the restaurant needs to work on reducing its wait times. This could involve streamlining the order taking and delivery process, or increasing staffing during peak hours.\n",
            "\n",
            "3. Value for money: The negative feedback on value for money suggests that the restaurant needs to work on offering more affordable prices or improving the quality of its dishes. This could involve sourcing more affordable ingredients, reevaluating menu pricing, or improving the overall quality of dishes.\n",
            "\n",
            "4. Menu variety: While the food quality has been rated positively, the restaurant may want to consider expanding its menu to include a wider variety of dishes. This could help attract more customers who are looking for diverse dining options.\n",
            "\n",
            "5. Marketing: Depending on the restaurant's location, the mixed feedback may suggest that it needs to work on improving its marketing efforts. This could involve increasing social media presence, running promotions or discounts, or partnering with local influencers to increase awareness of the restaurant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using router Chain**"
      ],
      "metadata": {
        "id": "jse42ce3L9x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define new prompt templates for the restaurant scenario\n",
        "feedback_categorization_template = \"\"\"You are a feedback categorization expert. Categorize the following customer feedback:\n",
        "\n",
        "{input}\"\"\"\n",
        "\n",
        "sentiment_summary_template = \"\"\"You are skilled in summarizing sentiment. Summarize the sentiment of this customer feedback:\n",
        "\n",
        "{input}\"\"\"\n",
        "\n",
        "improvement_identification_template = \"\"\"You are an expert in identifying areas for improvement from customer feedback. Identify what can be improved based on:\n",
        "\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "-tBD1UjHMj1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the prompt_infos with the new scenarios\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"feedback_categorization\",\n",
        "        \"description\": \"Categorizes customer feedback into aspects like service, food, and ambiance.\",\n",
        "        \"prompt_template\": feedback_categorization_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"sentiment_summary\",\n",
        "        \"description\": \"Summarizes the overall sentiment of the customer feedback.\",\n",
        "        \"prompt_template\": sentiment_summary_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"improvement_identification\",\n",
        "        \"description\": \"Identifies areas for improvement from the customer feedback.\",\n",
        "        \"prompt_template\": improvement_identification_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "XyZbVwQaMs6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chains for each prompt\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    prompt = ChatPromptTemplate.from_template(template=p_info[\"prompt_template\"])\n",
        "    destination_chains[p_info[\"name\"]] = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "0BXLif0uMzwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "-gT8vCVUNEVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the RouterPrompt and RouterChain\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template.format(destinations=\"\\n\".join(f\"{p['name']}: {p['description']}\" for p in prompt_infos)),\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "# Combine into a MultiPromptChain\n",
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True)"
      ],
      "metadata": {
        "id": "PK6XAZKVT1Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_feedback = \"The steak was amazing but the service was too slow. We also loved the ambiance of the place.\"\n",
        "\n",
        "# Run the MultiPromptChain with the customer feedback\n",
        "results = chain.run(customer_feedback)\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWI7taJNL6bh",
        "outputId": "9c2e3bcd-25ba-4544-b7f2-2d656cc52a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "feedback_categorization: {'input': 'The steak was amazing but the service was too slow. We also loved the ambiance of the place.'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Food Quality - Positive (amazing steak)\n",
            "Service - Negative (slow)\n",
            "Ambiance - Positive (loved ambiance)\n"
          ]
        }
      ]
    }
  ]
}